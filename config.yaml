model_config:
  model_name: "mini-aethergpt"
  maxlen: 1024
  vocab_size: 50257 # GPT-2 vocab size
  embed_dim: 256
  num_heads: 8
  feed_forward_dim: 256 # 4 * embed_dim
  num_transformer_blocks: 3
  dropout_rate: 0.1
  dropconnect_rate: 0.1
  use_dropconnect: False
  use_softermax: True
  power: 1.0
  use_activation: False
  use_yatnmn: False

data_config:
  dataset_name: "Kunj07/openwebtext"
  split: "train"
  batch_size: 256
  tokenizer_name: "gpt2"
  # Data loading configuration
  loader: "tf"  # 'grain' or 'tf'
  use_cache: true # Whether to cache the dataset. Applies to both loaders.
  shuffle_seed: 42
  shuffle_buffer_size: 10000
  cache_size: 100000 # Used only for the 'grain' loader's cache.
  num_threads: 4
  prefetch_buffer_size: 50
  tokenization_batch_size: 1000
  use_fast_tokenizer: true

train_config:
  optimizer_name: "adamw"
  num_epochs: 20
  max_iterations: 500000
  learning_rate: 0.0003
  lr_warmup_steps: 100
  lr_num_decay_steps: 500000
  weight_decay: 0.1
  grad_clip_value: 0.0 # Set to > 0.0 to enable
  log_interval: 100
  text_log_interval: 1000
  use_wandb: true
  checkpoint_dir: 'checkpoints'
  debug: false
  run_generation: true
  log_determinants: false
  log_gradients: false
  start_prompt: "The difference between the mind and the cosmos is"
  log_batch_stats: true
  log_batch_identity: true
  loss_function: "optax"  # 'optax' or 'softermax'
