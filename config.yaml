model_config:
  model_name: "mini-aethergpt"
  maxlen: 1024
  vocab_size: 50257 # GPT-2 vocab size
  embed_dim: 256
  num_heads: 8
  feed_forward_dim: 256 # 4 * embed_dim
  num_transformer_blocks: 3
  dropout_rate: 0.1
  dropconnect_rate: 0.1
  use_dropconnect: False
  use_softermax: True
  power: 1.0

data_config:
  dataset_name: "Kunj07/openwebtext"
  split: "train"
  batch_size: 256
  tokenizer_name: "gpt2"
  # Data loading configuration
  shuffle_seed: 42
  shuffle_buffer_size: 10000
  cache_size: 100000
  num_threads: 4
  prefetch_buffer_size: 50
  tokenization_batch_size: 1000
  use_fast_tokenizer: true
  loader: "tf"  # 'grain' or 'tf'

train_config:
  optimizer_name: "adamw"
  num_epochs: 20
  learning_rate: 0.0003
  lr_warmup_steps: 100
  lr_num_decay_steps: 500000
  weight_decay: 0.1
  grad_clip_value: 1.0
  log_interval: 100
  text_log_interval: 1000
  use_wandb: true
  checkpoint_dir: 'checkpoints'
  debug: false
  run_generation: true
  log_determinants: false
  log_gradients: false 